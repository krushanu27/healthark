{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5643e8a7-53ad-4a32-8513-fb57a6cadc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 0: SETUP\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6124c8-af7d-4d8a-9048-eac7eeb85595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PART 1: Loading and Preprocessing Initial Data ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 1: DATA LOADING AND PREPROCESSING\n",
    "# ==============================================================================\n",
    "print(\"--- PART 1: Loading and Preprocessing Initial Data ---\")\n",
    "\n",
    "code_columns = {\n",
    "    'ICD9_DGNS_CD_1': str, 'ICD9_DGNS_CD_2': str, 'ICD9_DGNS_CD_3': str,\n",
    "    'ICD9_DGNS_CD_4': str, 'ICD9_DGNS_CD_5': str, 'ICD9_DGNS_CD_6': str,\n",
    "    'ICD9_DGNS_CD_7': str, 'ICD9_DGNS_CD_8': str, 'ICD9_DGNS_CD_9': str,\n",
    "    'ICD9_DGNS_CD_10': str, 'ADMTNG_ICD9_DGNS_CD': str, 'CLM_DRG_CD': str,\n",
    "    'ICD9_PRCDR_CD_1': str, 'ICD9_PRCDR_CD_2': str, 'ICD9_PRCDR_CD_3': str,\n",
    "    'ICD9_PRCDR_CD_4': str, 'ICD9_PRCDR_CD_5': str, 'ICD9_PRCDR_CD_6': str\n",
    "}\n",
    "\n",
    "beneficiary_2008 = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2008_Beneficiary_Summary_File_Sample_1.csv\")\n",
    "beneficiary_2009 = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2009_Beneficiary_Summary_File_Sample_1.csv\")\n",
    "beneficiary_2010 = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2010_Beneficiary_Summary_File_Sample_1.csv\")\n",
    "\n",
    "chunk_size = 100000\n",
    "    \n",
    "inpatient_agg_list, inpatient_codes_list, inpatient_readmission_list = [], [], []\n",
    "inpatient_iterator = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv\", dtype=code_columns, chunksize=chunk_size)\n",
    "for chunk in inpatient_iterator:\n",
    "    inpatient_agg_list.append(chunk.groupby('DESYNPUF_ID').agg(Inpatient_Claim_Count=('CLM_ID', 'count'), Total_Inpatient_Payments=('CLM_PMT_AMT', 'sum')))\n",
    "    inpatient_codes_list.append(chunk[['DESYNPUF_ID', 'ICD9_DGNS_CD_1']])\n",
    "    chunk['CLM_ADMSN_DT'] = pd.to_datetime(chunk['CLM_ADMSN_DT'], format='%Y%m%d')\n",
    "    chunk['CLM_THRU_DT'] = pd.to_datetime(chunk['CLM_THRU_DT'], format='%Y%m%d', errors='coerce')\n",
    "    inpatient_readmission_list.append(chunk)\n",
    "\n",
    "inpatient_agg = pd.concat(inpatient_agg_list).groupby(level=0).sum()\n",
    "inpatient_codes = pd.concat(inpatient_codes_list)\n",
    "inpatient_claims_raw = pd.concat(inpatient_readmission_list)\n",
    "    \n",
    "outpatient_agg_list, outpatient_codes_list = [], []\n",
    "outpatient_iterator = pd.read_csv(\"D:/Jupyter/HealthArk_data/DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv\", dtype=code_columns, engine='python', chunksize=chunk_size)\n",
    "for chunk in outpatient_iterator:\n",
    "    outpatient_agg_list.append(chunk.groupby('DESYNPUF_ID').agg(Outpatient_Claim_Count=('CLM_ID', 'count'), Total_Outpatient_Payments=('CLM_PMT_AMT', 'sum')))\n",
    "    outpatient_codes_list.append(chunk[['DESYNPUF_ID', 'ICD9_DGNS_CD_1']])\n",
    "        \n",
    "outpatient_agg = pd.concat(outpatient_agg_list).groupby(level=0).sum()\n",
    "outpatient_codes = pd.concat(outpatient_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6dd95b8-c30b-4454-a8b5-6bef5cd2b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 2: Engineering Features and Merging Data ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 2: FEATURE ENGINEERING AND MERGING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 2: Engineering Features and Merging Data ---\")\n",
    "\n",
    "all_beneficiaries = pd.concat([beneficiary_2008, beneficiary_2009, beneficiary_2010], ignore_index=True)\n",
    "all_beneficiaries = all_beneficiaries.drop_duplicates(subset=['DESYNPUF_ID'], keep='last')\n",
    "    \n",
    "all_beneficiaries['BENE_BIRTH_DT'] = pd.to_datetime(all_beneficiaries['BENE_BIRTH_DT'], format='%m-%d-%Y')\n",
    "all_beneficiaries['BENE_DEATH_DT'] = pd.to_datetime(all_beneficiaries['BENE_DEATH_DT'], format='%m-%d-%Y', errors='coerce')\n",
    "reference_date = datetime(2010, 12, 31)\n",
    "all_beneficiaries['Age'] = ((reference_date - all_beneficiaries['BENE_BIRTH_DT']).dt.days / 365.25).astype(int)\n",
    "all_beneficiaries['Is_Dead'] = all_beneficiaries['BENE_DEATH_DT'].notna().astype(int)\n",
    "chronic_condition_cols = [col for col in all_beneficiaries.columns if col.startswith('SP_')]\n",
    "for col in chronic_condition_cols:\n",
    "    all_beneficiaries[col] = all_beneficiaries[col].replace(2, 0)\n",
    "all_beneficiaries['Chronic_Condition_Count'] = all_beneficiaries[chronic_condition_cols].sum(axis=1)\n",
    "    \n",
    "master_df = all_beneficiaries.merge(inpatient_agg, on='DESYNPUF_ID', how='left')\n",
    "master_df = master_df.merge(outpatient_agg, on='DESYNPUF_ID', how='left')\n",
    "claims_cols_to_fill = ['Inpatient_Claim_Count', 'Total_Inpatient_Payments', 'Outpatient_Claim_Count', 'Total_Outpatient_Payments']\n",
    "master_df[claims_cols_to_fill] = master_df[claims_cols_to_fill].fillna(0)\n",
    "\n",
    "inpatient_claims_raw = inpatient_claims_raw.sort_values(by=['DESYNPUF_ID', 'CLM_ADMSN_DT'])\n",
    "inpatient_claims_raw['Next_Admission_Date'] = inpatient_claims_raw.groupby('DESYNPUF_ID')['CLM_ADMSN_DT'].shift(-1)\n",
    "days_to_next_admission = (inpatient_claims_raw['Next_Admission_Date'] - inpatient_claims_raw['CLM_THRU_DT']).dt.days\n",
    "inpatient_claims_raw['Was_Readmitted_in_30_Days'] = (days_to_next_admission <= 30).astype(int)\n",
    "readmission_summary = inpatient_claims_raw.groupby('DESYNPUF_ID')['Was_Readmitted_in_30_Days'].max().reset_index()\n",
    "readmission_summary = readmission_summary.rename(columns={'Was_Readmitted_in_30_Days': 'Had_30Day_Readmission_Ever'})\n",
    "master_df_readmission = master_df.merge(readmission_summary, on='DESYNPUF_ID', how='left')\n",
    "master_df_readmission['Had_30Day_Readmission_Ever'] = master_df_readmission['Had_30Day_Readmission_Ever'].fillna(0)\n",
    "    \n",
    "all_codes = pd.concat([inpatient_codes, outpatient_codes], ignore_index=True)\n",
    "diagnosis_counts = all_codes.groupby('DESYNPUF_ID').size().reset_index(name='Total_Diagnosis_Count')\n",
    "unique_diagnosis_counts = all_codes.groupby('DESYNPUF_ID')['ICD9_DGNS_CD_1'].nunique().reset_index(name='Unique_Diagnosis_Count')\n",
    "master_df_enhanced = master_df_readmission.merge(diagnosis_counts, on='DESYNPUF_ID', how='left')\n",
    "master_df_enhanced = master_df_enhanced.merge(unique_diagnosis_counts, on='DESYNPUF_ID', how='left')\n",
    "master_df_enhanced[['Total_Diagnosis_Count', 'Unique_Diagnosis_Count']] = master_df_enhanced[['Total_Diagnosis_Count', 'Unique_Diagnosis_Count']].fillna(0)\n",
    "categorical_cols = ['BENE_SEX_IDENT_CD', 'BENE_RACE_CD']\n",
    "master_df_enhanced = pd.get_dummies(master_df_enhanced, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "try:\n",
    "    drug_exposure = pd.read_excel(\"D:/Jupyter/HealthArk_data/drug_exposure.xlsx\")\n",
    "    person_mapping = pd.read_excel(\"D:/Jupyter/HealthArk_data/person.xlsx\")\n",
    "    person_id_map = person_mapping[['PERSON_ID', 'PERSON_SOURCE_VALUE']].rename(columns={'PERSON_SOURCE_VALUE': 'DESYNPUF_ID'})\n",
    "    drug_exposure = drug_exposure.merge(person_id_map, on='PERSON_ID', how='left')\n",
    "    \n",
    "    if 'DESYNPUF_ID' in drug_exposure.columns:\n",
    "        drug_counts = drug_exposure.groupby('DESYNPUF_ID').size().reset_index(name='Total_Drug_Count')\n",
    "        unique_drug_counts = drug_exposure.groupby('DESYNPUF_ID')['DRUG_CONCEPT_ID'].nunique().reset_index(name='Unique_Drug_Count')\n",
    "        avg_days_supply = drug_exposure.groupby('DESYNPUF_ID')['DAYS_SUPPLY'].mean().reset_index(name='Avg_Days_Supply')\n",
    "        master_df_final = master_df_enhanced.merge(drug_counts, on='DESYNPUF_ID', how='left')\n",
    "        master_df_final = master_df_final.merge(unique_drug_counts, on='DESYNPUF_ID', how='left')\n",
    "        master_df_final = master_df_final.merge(avg_days_supply, on='DESYNPUF_ID', how='left')\n",
    "        drug_feature_cols = ['Total_Drug_Count', 'Unique_Drug_Count', 'Avg_Days_Supply']\n",
    "        master_df_final[drug_feature_cols] = master_df_final[drug_feature_cols].fillna(0)\n",
    "except FileNotFoundError:\n",
    "    print(\"Drug or Person files not found. Skipping drug features.\")\n",
    "    master_df_final = master_df_enhanced.copy()\n",
    "    master_df_final[['Total_Drug_Count', 'Unique_Drug_Count', 'Avg_Days_Supply']] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b847a2-f0fd-4268-a007-964d933777b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 03m 53s]\n",
      "val_recall: 1.0\n",
      "\n",
      "Best val_recall So Far: 1.0\n",
      "Total elapsed time: 00h 35m 28s\n",
      "\n",
      "Search complete. The optimal hyperparameters are:\n",
      "- Neurons in first layer: 128\n",
      "- Dropout rate: 0.20\n",
      "- Neurons in second layer: 64\n",
      "- Learning rate: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 3: NEURAL NETWORK DATA PREPARATION AND TUNING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 3: Preparing Data and Tuning the Neural Network ---\")\n",
    "\n",
    "df_for_nn = master_df_final.copy()\n",
    "y = df_for_nn['Had_30Day_Readmission_Ever']\n",
    "features_to_drop = ['DESYNPUF_ID', 'BENE_BIRTH_DT', 'BENE_DEATH_DT', 'Had_30Day_Readmission_Ever']\n",
    "X = df_for_nn.drop(columns=features_to_drop)\n",
    "X = X.select_dtypes(include=['number'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=128, step=32)\n",
    "    model.add(Dense(units=hp_units_1, activation='relu'))\n",
    "    hp_dropout_1 = hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(hp_dropout_1))\n",
    "    hp_units_2 = hp.Int('units_2', min_value=16, max_value=64, step=16)\n",
    "    model.add(Dense(units=hp_units_2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['recall'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                        objective='val_recall',\n",
    "                        max_trials=10,\n",
    "                        executions_per_trial=2,\n",
    "                        directory='keras_tuner_dir_nn',\n",
    "                        project_name='readmission_tuning_nn')\n",
    "\n",
    "neg_cases, pos_cases = y_train.value_counts()\n",
    "total = neg_cases + pos_cases\n",
    "weight_for_0 = (1 / neg_cases) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos_cases) * (total / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(\"\\n--- Starting the Search for the Best Neural Network Model ---\")\n",
    "tuner.search(X_train_scaled, y_train, epochs=10, validation_split=0.2, class_weight=class_weight)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Search complete. The optimal hyperparameters are:\n",
    "- Neurons in first layer: {best_hps.get('units_1')}\n",
    "- Dropout rate: {best_hps.get('dropout_1'):.2f}\n",
    "- Neurons in second layer: {best_hps.get('units_2')}\n",
    "- Learning rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27adcfc8-be2f-4943-9544-0d1bf37b516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PART 4: Training the Best Model on the Full Training Data ---\n",
      "\n",
      "Saved trained Neural Network to 'neural_network_champion_model.keras'\n",
      "Epoch 1/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.2269 - recall: 0.9695 - val_loss: 0.2347 - val_recall: 0.9907\n",
      "Epoch 2/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.2007 - recall: 0.9851 - val_loss: 0.1985 - val_recall: 0.9969\n",
      "Epoch 3/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.1678 - recall: 0.9906 - val_loss: 0.2430 - val_recall: 0.9990\n",
      "Epoch 4/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.1927 - recall: 0.9876 - val_loss: 0.1812 - val_recall: 1.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.2139 - recall: 0.9945 - val_loss: 0.1776 - val_recall: 1.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.2318 - recall: 0.9940 - val_loss: 0.2569 - val_recall: 0.9979\n",
      "Epoch 7/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 0.2018 - recall: 0.9950 - val_loss: 0.2365 - val_recall: 1.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.1719 - recall: 0.9973 - val_loss: 0.4065 - val_recall: 0.9990\n",
      "Epoch 9/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.1783 - recall: 0.9963 - val_loss: 0.2197 - val_recall: 1.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.2127 - recall: 0.9958 - val_loss: 0.2156 - val_recall: 0.9979\n",
      "Epoch 11/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.2223 - recall: 0.9943 - val_loss: 0.2040 - val_recall: 0.9979\n",
      "Epoch 12/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.2036 - recall: 0.9943 - val_loss: 0.2371 - val_recall: 0.9866\n",
      "Epoch 13/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.2081 - recall: 0.9968 - val_loss: 0.2363 - val_recall: 1.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.2785 - recall: 0.9926 - val_loss: 0.2550 - val_recall: 0.9990\n",
      "Epoch 15/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.2292 - recall: 0.9965 - val_loss: 0.2512 - val_recall: 0.9928\n",
      "Epoch 16/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.2455 - recall: 0.9948 - val_loss: 0.2014 - val_recall: 1.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.2035 - recall: 0.9968 - val_loss: 0.2034 - val_recall: 1.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.1956 - recall: 0.9970 - val_loss: 0.1955 - val_recall: 1.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.2316 - recall: 0.9973 - val_loss: 0.2036 - val_recall: 1.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m2327/2327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.1900 - recall: 0.9978 - val_loss: 0.1993 - val_recall: 1.0000\n",
      "\n",
      "--- Evaluating the Tuned Neural Network Performance ---\n",
      "\u001b[1m728/728\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\n",
      "Classification Report (Tuned Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.92      0.96     22021\n",
      "         1.0       0.40      1.00      0.57      1250\n",
      "\n",
      "    accuracy                           0.92     23271\n",
      "   macro avg       0.70      0.96      0.77     23271\n",
      "weighted avg       0.97      0.92      0.94     23271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART 4: TRAINING AND EVALUATING THE BEST NEURAL NETWORK\n",
    "# ==============================================================================\n",
    "print(\"\\n--- PART 4: Training the Best Model on the Full Training Data ---\")\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "best_model.save('neural_network_champion_model.keras')\n",
    "print(\"\\nSaved trained Neural Network to 'neural_network_champion_model.keras'\")\n",
    "history = best_model.fit(X_train_scaled, y_train, epochs=20, validation_split=0.2, class_weight=class_weight)\n",
    "\n",
    "print(\"\\n--- Evaluating the Tuned Neural Network Performance ---\")\n",
    "y_probs = best_model.predict(X_test_scaled)\n",
    "y_pred = (y_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report (Tuned Neural Network):\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4040b-c09f-4a3e-93fa-4dab606e8e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
